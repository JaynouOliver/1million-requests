{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey\n"
     ]
    }
   ],
   "source": [
    "print(\"hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Subhr\\Documents\\github\\curieoAI\\bert\\sentiment_analysis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully converted to ONNX format.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Define the model path and load the model\n",
    "model_path = \"./model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the dummy input for the model\n",
    "dummy_input = tokenizer(\"This is a dummy input\", return_tensors=\"pt\")\n",
    "\n",
    "# Define the input and output names for the ONNX model\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "output_names = [\"output\"]\n",
    "\n",
    "# Export the model with opset version 14\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]), \n",
    "    \"model.onnx\", \n",
    "    input_names=input_names, \n",
    "    output_names=output_names, \n",
    "    opset_version=14, \n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Model has been successfully converted to ONNX format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the model path\n",
    "model_path = \"./model.onnx\"\n",
    "tokenizer_path = \"./model\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Load the ONNX model\n",
    "ort_session = ort.InferenceSession(model_path)\n",
    "\n",
    "# Define the input schema\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "def preprocess(text: str):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"np\", padding=True, truncation=True)\n",
    "    return inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "\n",
    "def predict_onnx(text: str):\n",
    "    # Preprocess the input text\n",
    "    input_ids, attention_mask = preprocess(text)\n",
    "    \n",
    "    # Run the ONNX model\n",
    "    ort_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    # Get the output\n",
    "    return ort_outs[0]\n",
    "\n",
    "# Define the predict endpoint\n",
    "@app.post(\"/predict\")\n",
    "def predict(input: TextInput):\n",
    "    # Perform the prediction\n",
    "    logits = predict_onnx(input.text)\n",
    "    prediction = np.argmax(logits, axis=1).item()\n",
    "    return {\"prediction\": prediction}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been further optimized and saved.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.transformers import optimizer\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"model.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Optimize the ONNX model\n",
    "optimized_model = optimizer.optimize_model(onnx_model, model_type='bert')\n",
    "\n",
    "# Save the optimized model\n",
    "optimized_model_path = \"model2.onnx\"\n",
    "optimized_model.save_model_to_file(optimized_model_path)\n",
    "\n",
    "print(\"Model has been further optimized and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Define the path to your local model directory\n",
    "model_dir = \"model/\"\n",
    "\n",
    "# Load the tokenizer and model from the local directory\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Test the model\n",
    "text = \"This is a great movie!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class_id = torch.argmax(logits).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to convert to ONNX models from pytorch \n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model_path = \"./model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "# Dummy input for tracing the model\n",
    "dummy_input = tokenizer(\"This is a dummy input\", return_tensors=\"pt\")\n",
    "\n",
    "# Convert the model to ONNX\n",
    "torch.onnx.export(model, \n",
    "                  (dummy_input['input_ids'], dummy_input['attention_mask']), \n",
    "                  \"model.onnx\",\n",
    "                  input_names=['input_ids', 'attention_mask'], \n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, 'attention_mask': {0: 'batch_size'}, 'output': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup onnx code \n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, TextClassificationPipeline, BertTokenizerFast\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "local_model_path = \"./model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(local_model_path)\n",
    "onnx_model_path = \"model.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "#for pytorch\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "# pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "class TextInput(BaseModel):\n",
    "    text: str\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "#for running with pytorch\n",
    "# @app.post(\"/classify/\")\n",
    "# def classify_text(input: TextInput):\n",
    "#     result = pipeline(input.text)\n",
    "#     return {\"label\": result[0]['label'], \"score\": result[0]['score']}\n",
    "\n",
    "\n",
    "#for onnx\n",
    "@app.post(\"/classify/\")\n",
    "def classify_text(input: TextInput):\n",
    "    inputs = tokenizer(input.text, return_tensors=\"pt\")\n",
    "    ort_inputs = {k: to_numpy(v) for k, v in inputs.items()}\n",
    "    ort_outputs = ort_session.run(None, ort_inputs)\n",
    "    output = ort_outputs[0]\n",
    "    label = np.argmax(output, axis=1)[0]\n",
    "    score = np.max(output, axis=1)[0]\n",
    "    return {\"label\": int(label), \"score\": float(score)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
